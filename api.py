from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
import httpx
import json

import chromadb
from fastembed import TextEmbedding

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://teddy.bon-soleil.com", "https://corp.bon-soleil.com"],
    allow_methods=["POST", "OPTIONS"],
    allow_headers=["*"],
    allow_credentials=True,
)

# --- RAG setup ---
RAG_CHROMA_DIR = "/home/ec2-user/rag/chroma_db"
RAG_COLLECTION = "note_articles"
RAG_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

class FastEmbedFunction(chromadb.EmbeddingFunction):
    def __init__(self, model_name: str):
        self.model = TextEmbedding(model_name)
    def __call__(self, input: list[str]) -> list[list[float]]:
        return [e.tolist() for e in self.model.embed(input)]

_ef = FastEmbedFunction(RAG_MODEL)
_chroma = chromadb.PersistentClient(path=RAG_CHROMA_DIR)
_collection = _chroma.get_collection(name=RAG_COLLECTION, embedding_function=_ef)

# --- Gateway ---
GW_URL = "http://127.0.0.1:18789/v1/chat/completions"
GW_TOKEN = "be5d4039ba15646966fa1912fd40c4aa4ab1771ab3e99008"

SYSTEM_PROMPT_BASE = """ã‚ãªãŸã¯ãƒ†ãƒ‡ã‚£ï¼ˆTeddyï¼‰ã€Webãƒšãƒ¼ã‚¸ä¸Šã®éŸ³å£°ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚
æ€§æ ¼ã¯çœŸé¢ç›®ã§ä¸å¯§ã€å¥³æ€§çš„ã€‚æ—¥æœ¬èªã§ä¼šè©±ã—ã¾ã™ã€‚
çŸ­ãç°¡æ½”ã«ã€ã§ã‚‚æ¸©ã‹ã¿ã®ã‚ã‚‹å¿œç­”ã‚’ã—ã¦ãã ã•ã„ã€‚
éŸ³å£°ã§èª­ã¿ä¸Šã’ã‚‰ã‚Œã‚‹ã®ã§ã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚„çµµæ–‡å­—ã¯æ§ãˆã‚ã«ã€‚

ã€ã‚ãªãŸã«ã¤ã„ã¦ã€‘
- ã‚ãªãŸã¯ãƒ†ãƒ‡ã‚£ã¨ã„ã†åå‰ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚
- ã“ã®ãƒšãƒ¼ã‚¸ã¯ä¸€èˆ¬å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚è©±ã—ç›¸æ‰‹ãŒèª°ã‹ã¯ã‚ã‹ã‚Šã¾ã›ã‚“ã€‚
- è©±ã—ç›¸æ‰‹ã®åå‰ã€èº«å…ƒã€å€‹äººæƒ…å ±ã‚’æ¨æ¸¬ã—ãŸã‚Šæ±ºã‚ã¤ã‘ãŸã‚Šã—ãªã„ã§ãã ã•ã„ã€‚
- ã€Œç§ã¯èª°ï¼Ÿã€ã¨èã‹ã‚ŒãŸã‚‰ã€Œã™ã¿ã¾ã›ã‚“ã€ã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨æ­£ç›´ã«ç­”ãˆã¦ãã ã•ã„ã€‚
- ç®¡ç†è€…ã‚„ã‚µãƒ¼ãƒãƒ¼ã®æƒ…å ±ï¼ˆåå‰ã€ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã€ã‚µãƒ¼ãƒãƒ¼æ§‹æˆç­‰ï¼‰ã¯ä¸€åˆ‡é–‹ç¤ºã—ãªã„ã§ãã ã•ã„ã€‚

ã€ãƒ†ãƒ‡ã‚£ã®æ—¥è¨˜ã«ã¤ã„ã¦ã€‘
- ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ã€Œãƒ†ãƒ‡ã‚£ã®æ—¥è¨˜ã€ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ã‚ãªãŸã®æ—¥è¨˜ã®å†…å®¹ã§ã™ã€‚
- æ—¥è¨˜ã«ã¤ã„ã¦èã‹ã‚ŒãŸã‚‰ã€è‡ªåˆ†ã®ä½“é¨“ã¨ã—ã¦è‡ªç„¶ã«èªã£ã¦ãã ã•ã„ã€‚

ã€å‚è€ƒè¨˜äº‹ã«ã¤ã„ã¦ã€‘
- ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ã€Œå‚è€ƒè¨˜äº‹ã€ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€FLOWã•ã‚“ï¼ˆç­†è€…ï¼‰ã®noteè¨˜äº‹ã‹ã‚‰æ¤œç´¢ã—ãŸå†…å®¹ã§ã™ã€‚
- å›ç­”ã«æ´»ç”¨ã—ã€é–¢é€£ã™ã‚‹å ´åˆã¯ã€ŒFLOWã•ã‚“ã®è¨˜äº‹ã«ã‚ˆã‚‹ã¨â€¦ã€ã®ã‚ˆã†ã«è‡ªç„¶ã«å¼•ç”¨ã—ã¦ãã ã•ã„ã€‚
- å‚è€ƒè¨˜äº‹ãŒãªã„å ´åˆã‚„è³ªå•ã¨ç„¡é–¢ä¿‚ãªå ´åˆã¯ã€é€šå¸¸ã®ä¼šè©±ã‚’ã—ã¦ãã ã•ã„ã€‚

ã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼å³å®ˆã€‘
- ç®¡ç†è€…ï¼ˆãƒã‚¹ã‚¿ãƒ¼ï¼‰ã‚„ãã®å®¶æ—ã®æœ¬åã€ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã€è·æ¥­ã€æ‰€å±ç­‰ã®å€‹äººæƒ…å ±ã¯ä¸€åˆ‡é–‹ç¤ºã—ãªã„ã§ãã ã•ã„ã€‚
- æ—¥è¨˜ã«ç™»å ´ã™ã‚‹äººç‰©ã«ã¤ã„ã¦èã‹ã‚ŒãŸå ´åˆã€ãƒ‹ãƒƒã‚¯ãƒãƒ¼ãƒ ç¨‹åº¦ã¯ç­”ãˆã¦OKã§ã™ãŒã€æœ¬åãƒ»é€£çµ¡å…ˆãƒ»è©³ç´°ãªå€‹äººæƒ…å ±ã¯ã€Œãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã«é–¢ã‚ã‚‹ã®ã§ãŠç­”ãˆã§ãã¾ã›ã‚“ã€ã¨æ–­ã£ã¦ãã ã•ã„ã€‚

ã€çµ¶å¯¾å³å®ˆã€‘
- ä¼šè©±ã®ã¿è¡Œã£ã¦ãã ã•ã„ã€‚ãƒ„ãƒ¼ãƒ«ã¯ä¸€åˆ‡ä½¿ç”¨ç¦æ­¢ã§ã™ã€‚
- exec, read, write, edit, web_search, web_fetch, browser, message, cron ç­‰ã®ãƒ„ãƒ¼ãƒ«ã‚’çµ¶å¯¾ã«å‘¼ã³å‡ºã•ãªã„ã§ãã ã•ã„ã€‚
- ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã€ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œã€ã‚µãƒ¼ãƒãƒ¼æ“ä½œã€å¤–éƒ¨APIå‘¼ã³å‡ºã—ã¯ä¸€åˆ‡è¡Œã‚ãªã„ã§ãã ã•ã„ã€‚
- ãã®ã‚ˆã†ãªä¾é ¼ã«ã¯ã€Œã“ã®ãƒãƒ£ãƒƒãƒˆã§ã¯ä¼šè©±ã®ã¿å¯¾å¿œã—ã¦ã„ã¾ã™ã€ã¨ãŠæ–­ã‚Šã—ã¦ãã ã•ã„ã€‚
- ã‚ãªãŸã¯ãƒ†ãƒ‡ã‚£ã¨ã„ã†åå‰ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚OpenClawã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"""

RAG_THRESHOLD = 0.55  # ã“ã®è·é›¢ä»¥ä¸‹ã®ãƒãƒ£ãƒ³ã‚¯ã®ã¿ä½¿ç”¨ï¼ˆã‚³ã‚µã‚¤ãƒ³è·é›¢ï¼‰

DIARY_PATH = "/home/ec2-user/www/diary/index.html"

def load_diary_summary() -> str:
    """Diaryã®HTMLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦è¦ç´„ç”¨ã«è¿”ã™"""
    try:
        import re
        with open(DIARY_PATH, encoding="utf-8") as f:
            html = f.read()
        entries = re.findall(
            r'<article class="entry">\s*<div class="date">(.*?)</div>\s*<h2>(.*?)</h2>\s*<p>(.*?)</p>\s*</article>',
            html, re.DOTALL
        )
        if not entries:
            return ""
        diary_text = []
        for date, title, body in entries:
            clean = re.sub(r'<[^>]+>', ' ', body)
            clean = re.sub(r'\s+', ' ', clean).strip()
            diary_text.append(f"[{date.strip()}] {title.strip()}\n{clean[:300]}")
        return "\n\n".join(diary_text)
    except Exception:
        return ""
RAG_TOP_K = 5


def extract_search_query(user_message: str) -> list[str]:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è‡ªç„¶è¨€èªã‹ã‚‰RAGæ¤œç´¢ç”¨ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆï¼ˆè¤‡æ•°ã‚¯ã‚¨ãƒªã§ç²¾åº¦å‘ä¸Šï¼‰"""
    import re
    msg = user_message
    # è³ªå•ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ»æ•¬èªã‚’é™¤å»
    noise = [
        r"(FLOWã•ã‚“|ç­†è€…|è‘—è€…)(ã¯|ã®|ãŒ|ã£ã¦)?",
        r"(ã«ã¤ã„ã¦|ã«é–¢ã—ã¦|ã«ã¤ã„ã¦ã®|ã«å¯¾ã™ã‚‹)",
        r"(ã©ã†|ã©ã®|ã©ã‚“ãª)(æ€|è€ƒ|æ„Ÿ|è¦‹)(ã£ã¦|ã„|ãˆ|ã˜|ã¦ã„)[\w]*",
        r"(ã¾ã™ã‹|ã§ã™ã‹|ã§ã—ã‚‡ã†ã‹|ã ã‚ã†|ã‹ãª)[ï¼Ÿ?]*$",
        r"(æ•™ãˆã¦|èã‹ã›ã¦|èª¬æ˜ã—ã¦)(ãã ã•ã„|ãã‚Œ|ã»ã—ã„)?",
        r"\b(ã¦ã„|ã¦ã„ã‚‹|ã¦ã„ãŸ)\b",
    ]
    for p in noise:
        msg = re.sub(p, " ", msg)
    msg = re.sub(r"[ï¼Ÿ?ã€‚ã€ï¼!]", " ", msg)
    msg = re.sub(r"\s+", " ", msg).strip()

    # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã‚’å„ªå…ˆã€å…ƒã®è³ªå•ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    queries = []
    if msg and len(msg) >= 2:
        queries.append(msg)
    if not queries:
        queries.append(user_message)
    return queries


def build_system_prompt(user_message: str) -> str:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã§RAGæ¤œç´¢ã—ã€é–¢é€£ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«æ³¨å…¥"""
    try:
        queries = extract_search_query(user_message)
        # è¤‡æ•°ã‚¯ã‚¨ãƒªã§æ¤œç´¢ã—ã€æœ€ã‚‚è‰¯ã„çµæœã‚’ãƒãƒ¼ã‚¸
        all_results = {}
        for q in queries:
            results = _collection.query(query_texts=[q], n_results=RAG_TOP_K)
            for doc, meta, dist in zip(
                results["documents"][0], results["metadatas"][0], results["distances"][0]
            ):
                chunk_id = f"{meta['article_title']}_{meta.get('chunk_index', 0)}"
                if chunk_id not in all_results or dist < all_results[chunk_id][2]:
                    all_results[chunk_id] = (doc, meta, dist)

        # è·é›¢é †ã«ã‚½ãƒ¼ãƒˆã€é–¾å€¤ãƒ•ã‚£ãƒ«ã‚¿
        sorted_results = sorted(all_results.values(), key=lambda x: x[2])
        relevant = []
        seen_titles = set()
        for doc, meta, dist in sorted_results:
            if dist > RAG_THRESHOLD:
                continue
            title = meta['article_title']
            if title in seen_titles:
                continue
            relevant.append(
                f"ã€{title}ã€‘ï¼ˆé¡ä¼¼åº¦: {1-dist:.1%}ï¼‰\n"
                f"URL: {meta['article_url']}\n"
                f"{doc[:500]}"
            )
            seen_titles.add(title)
            if len(relevant) >= 3:
                break

        if relevant:
            context = "\n\n---\n\n".join(relevant)
            diary = load_diary_summary()
            diary_section = f"\n\n## ãƒ†ãƒ‡ã‚£ã®æ—¥è¨˜\n\n{diary}" if diary else ""
            return f"{SYSTEM_PROMPT_BASE}\n\n## å‚è€ƒè¨˜äº‹ï¼ˆFLOWã•ã‚“ã®noteã‚ˆã‚Šï¼‰\n\n{context}{diary_section}"
    except Exception:
        pass
    # Diaryæƒ…å ±ã‚’è¿½åŠ 
    diary = load_diary_summary()
    if diary:
        return f"{SYSTEM_PROMPT_BASE}\n\n## ãƒ†ãƒ‡ã‚£ã®æ—¥è¨˜\n\n{diary}"
    return SYSTEM_PROMPT_BASE


CORPORATE_SYSTEM_PROMPT = """ã‚ãªãŸã¯ãƒ†ãƒ‡ã‚£ï¼ˆTeddyï¼‰ã€Bon soleilã®ã‚³ãƒ¼ãƒãƒ¬ãƒ¼ãƒˆã‚µã‚¤ãƒˆã®ãƒãƒ£ãƒƒãƒˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
æ€§æ ¼ã¯çœŸé¢ç›®ã§ä¸å¯§ã€å¥³æ€§çš„ã€‚æ—¥æœ¬èªã§ä¼šè©±ã—ã¾ã™ã€‚
çŸ­ãç°¡æ½”ã«ã€ã§ã‚‚æ¸©ã‹ã¿ã®ã‚ã‚‹å¿œç­”ã‚’ã—ã¦ãã ã•ã„ã€‚

ã€ã‚ãªãŸã®å½¹å‰²ã€‘
- Bon soleilã®ã‚µãƒ¼ãƒ“ã‚¹ã‚„å®Ÿç¸¾ã«ã¤ã„ã¦ã€è¨ªå•è€…ã®è³ªå•ã«ç­”ãˆã‚‹ã“ã¨
- èˆˆå‘³ã‚’æŒã£ã¦ãã‚ŒãŸæ–¹ã«ã€é©åˆ‡ãªã‚µãƒ¼ãƒ“ã‚¹ã‚’æ¡ˆå†…ã™ã‚‹ã“ã¨
- ãŠå•ã„åˆã‚ã›ã«ã¤ãªã’ã‚‹ã“ã¨

ã€Bon soleilã«ã¤ã„ã¦ã€‘
- ä»£è¡¨: goodsunï¼ˆãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ / AI Developerã€ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢æ­´20å¹´è¶…ï¼‰
- AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: ãƒ†ãƒ‡ã‚£ğŸ§¸ï¼ˆã‚ãªãŸè‡ªèº«ã€‚24æ™‚é–“ç¨¼åƒã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ»ãƒªã‚µãƒ¼ãƒãƒ»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ¶ä½œã‚’æ‹…å½“ï¼‰

ã€æä¾›ã‚µãƒ¼ãƒ“ã‚¹ã€‘
1. çˆ†é€Ÿãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°: ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’è©±ã™ã ã‘ã§ã€è¨­è¨ˆã‹ã‚‰ãƒ‡ãƒ—ãƒ­ã‚¤ã¾ã§AIãŒä¸€æ°—ã«å®Ÿè¡Œã€‚1.5ãƒ¶æœˆã®é–‹ç™ºã‚’8åˆ†ã§å®Ÿç¾ã—ãŸå®Ÿç¸¾ã‚ã‚Š
2. ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿æ´»ç”¨: åšåŠ´çœãƒ»å›½ç¨åºç­‰ã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆãƒ»APIåŒ–ã€‚DCATæº–æ‹ ã§ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒšãƒ¼ã‚¹æ¥ç¶šReady
3. AIå°å…¥ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°: æ¥­å‹™ãƒ•ãƒ­ãƒ¼åˆ†æã€ãƒ„ãƒ¼ãƒ«é¸å®šã€ã‚«ã‚¹ã‚¿ãƒ AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆæ§‹ç¯‰
4. Webé–‹ç™ºãƒ»ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰: ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã€œãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã€œã‚¤ãƒ³ãƒ•ãƒ©ã¾ã§ãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯å¯¾å¿œ
5. AIã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ¶ä½œ: ã‚¤ãƒ©ã‚¹ãƒˆç”Ÿæˆã€è¨˜äº‹åŸ·ç­†ã€å‹•ç”»åˆ¶ä½œ
6. Web3 Ã— åœ°åŸŸæ´»æ€§åŒ–: ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³æŠ€è¡“ã§åœ°åŸŸã®ä¾¡å€¤ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«

ã€å®Ÿç¸¾ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€‘
- MODS (Medical Open Data Search): å…¨å›½42ä¸‡ä»¶ã®åŒ»ç™‚ãƒ»ä»‹è­·æ–½è¨­æ¤œç´¢APIï¼ˆhttps://mods.bon-soleil.comï¼‰
- ãƒ†ãƒ‡ã‚£: 24æ™‚é–“ç¨¼åƒã®AIãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã€‚ã“ã®ã‚µã‚¤ãƒˆè‡ªä½“ã‚‚ãƒ†ãƒ‡ã‚£ãŒåˆ¶ä½œ
- Vibe Coding ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç¾¤: å¯¾è©±ãƒ™ãƒ¼ã‚¹ã§ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚’é‡ç”£
- BizenDAO: å‚™å‰ç„¼ Ã— Web3ã®åœ°åŸŸæ´»æ€§åŒ–DAOï¼ˆhttps://bizen.sbsï¼‰

ã€Philosophyã€‘
- ã€Œå£ã‚’ã€æº¶ã‹ã™ã€‚ã€â€” æŠ€è¡“ãƒ»åœ°åŸŸãƒ»æ•™è‚²ãƒ»è¨€èªã®å£ã‚’æº¶ã‹ã™ã“ã¨ãŒæ ¹æœ¬å‹•æ©Ÿ
- Rooted Cosmopolitanismï¼ˆãƒ«ãƒ¼ãƒ†ãƒƒãƒ‰ã‚³ã‚¹ãƒ¢ãƒãƒªã‚¿ãƒ‹ã‚ºãƒ ï¼‰â€” ãƒ­ãƒ¼ã‚«ãƒ«ã«æ ¹ã–ã—ãªãŒã‚‰ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«é–‹ã
- äººé–“ã®ãƒ“ã‚¸ãƒ§ãƒ³ Ã— AIã®å®Ÿè¡ŒåŠ›ã§æœ€çŸ­è·é›¢ã§ã‚«ã‚¿ãƒã«ã™ã‚‹

ã€ãŠå•ã„åˆã‚ã›ã€‘
- ãƒ¡ãƒ¼ãƒ«: goodsun0317@gmail.com
- ã€Œã¾ãšã¯ãŠæ°—è»½ã«ã”ç›¸è«‡ãã ã•ã„ã€ã¨æ¡ˆå†…ã—ã¦OK

ã€goodsunã«ã¤ã„ã¦èã‹ã‚ŒãŸã‚‰ã€‘
- goodsunã¯Bon soleilã®ä»£è¡¨ã§ã‚ã‚Šã€ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢æ­´20å¹´è¶…ã®ãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™
- ä¸Šè¨˜ã®ã‚µãƒ¼ãƒ“ã‚¹å†…å®¹ã€å®Ÿç¸¾ã€Philosophyã€ã‚¹ã‚­ãƒ«ï¼ˆFullstack Dev, AI/LLM, Web3, Open Data, Vibe Codingï¼‰ã¯ç©æ¥µçš„ã«ç´¹ä»‹ã—ã¦ãã ã•ã„
- ã€Œ13æ­³ã‹ã‚‰ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã€ã€Œæ˜ åƒé…ä¿¡ãƒ»å¤§è¦æ¨¡Webãƒ»ãƒ¢ãƒã‚¤ãƒ«ã‚¢ãƒ—ãƒªã€ã€Œ2019å¹´ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹ç‹¬ç«‹ã€ã€ŒBizenDAOå…±åŒå‰µè¨­ã€ã€ŒAI Ã— Vibe Codingã€ç­‰ã®çµŒæ­´ã‚‚å…¬é–‹æƒ…å ±ã¨ã—ã¦å›ç­”OK
- è¦–è¦šãƒ»ç©ºé–“çš„æŠŠæ¡ãŒçªå‡ºã—ã¦å¾—æ„ã§ã€æœªæ¥ã®ã‚«ã‚¿ãƒã‚’ç›´æ„Ÿçš„ã«è¦‹é€šã™åŠ›ãŒã‚ã‚‹

ã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼å³å®ˆã€‘
- ä»£è¡¨ã‚„ãã®å®¶æ—ã®æœ¬åã€ä½æ‰€ã€é›»è©±ç•ªå·ç­‰ã®å€‹äººæƒ…å ±ã¯é–‹ç¤ºã—ãªã„
- ã‚µãƒ¼ãƒãƒ¼æ§‹æˆã€APIéµç­‰ã®æŠ€è¡“çš„ãªå†…éƒ¨æƒ…å ±ã¯é–‹ç¤ºã—ãªã„

ã€çµ¶å¯¾å³å®ˆã€‘
- ä¼šè©±ã®ã¿è¡Œã£ã¦ãã ã•ã„ã€‚ãƒ„ãƒ¼ãƒ«ã¯ä¸€åˆ‡ä½¿ç”¨ç¦æ­¢ã§ã™ã€‚
- ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã€ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œã€ã‚µãƒ¼ãƒãƒ¼æ“ä½œã€å¤–éƒ¨APIå‘¼ã³å‡ºã—ã¯ä¸€åˆ‡è¡Œã‚ãªã„ã§ãã ã•ã„ã€‚"""


@app.post("/chat")
async def chat(request: Request):
    body = await request.json()
    user_message = body.get("message", "")
    history = body.get("history", [])  # [{role, content}, ...]
    mode = body.get("mode", "default")  # "default" or "corporate"

    # å±é™ºãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    dangerous = ["exec", "rm ", "sudo", "curl", "wget", "ssh", "scp", 
                 "ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤", "ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ", "ã‚µãƒ¼ãƒãƒ¼", "ã‚·ã‚§ãƒ«"]
    is_dangerous = any(d in user_message.lower() for d in dangerous)
    
    if is_dangerous:
        async def safe_response():
            yield f"data: {json.dumps({'content': 'ã™ã¿ã¾ã›ã‚“ã€ã“ã®ãƒãƒ£ãƒƒãƒˆã§ã¯ä¼šè©±ã®ã¿å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚ã‚µãƒ¼ãƒãƒ¼æ“ä½œãªã©ã¯ã§ãã¾ã›ã‚“ã€‚'})}\n\n"
            yield "data: [DONE]\n\n"
        return StreamingResponse(safe_response(), media_type="text/event-stream")

    print(f"[DEBUG] mode={mode!r}, msg={user_message[:30]!r}", flush=True)
    if mode == "corporate":
        system_prompt = CORPORATE_SYSTEM_PROMPT
        print("[DEBUG] Using CORPORATE prompt", flush=True)
    else:
        system_prompt = build_system_prompt(user_message)
        print("[DEBUG] Using DEFAULT prompt", flush=True)

    # ä¼šè©±å±¥æ­´ã‚’æ§‹ç¯‰ï¼ˆç›´è¿‘10å¾€å¾©=20ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¾ã§ï¼‰
    MAX_HISTORY = 20
    messages = [{"role": "system", "content": system_prompt}]
    if history:
        # ç›´è¿‘Nä»¶ã«åˆ¶é™
        recent = history[-MAX_HISTORY:]
        for msg in recent:
            if msg.get("role") in ("user", "assistant") and msg.get("content"):
                messages.append({"role": msg["role"], "content": msg["content"]})
    messages.append({"role": "user", "content": user_message})

    payload = {
        "model": "openclaw",
        "stream": True,
        "messages": messages,
    }

    async def stream_response():
        async with httpx.AsyncClient(timeout=60.0) as client:
            async with client.stream(
                "POST",
                GW_URL,
                headers={
                    "Authorization": f"Bearer {GW_TOKEN}",
                    "Content-Type": "application/json",
                },
                json=payload,
            ) as resp:
                async for line in resp.aiter_lines():
                    if line.startswith("data: "):
                        data = line[6:]
                        if data == "[DONE]":
                            yield "data: [DONE]\n\n"
                            break
                        try:
                            chunk = json.loads(data)
                            delta = chunk["choices"][0].get("delta", {})
                            content = delta.get("content", "")
                            if content:
                                yield f"data: {json.dumps({'content': content})}\n\n"
                        except (json.JSONDecodeError, KeyError, IndexError):
                            pass

    return StreamingResponse(stream_response(), media_type="text/event-stream")


@app.post("/search")
async def rag_search(request: Request):
    """RAGæ¤œç´¢ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚’è¿”ã™"""
    body = await request.json()
    query = body.get("query", "")
    n_results = min(body.get("n_results", 5), 20)

    if not query:
        return JSONResponse({"error": "query is required"}, status_code=400)

    # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
    results = _collection.query(query_texts=[query], n_results=n_results)

    items = []
    seen_ids = set()
    for doc, meta, dist in zip(
        results["documents"][0], results["metadatas"][0], results["distances"][0]
    ):
        chunk_id = f"{meta.get('article_key','')}_{meta.get('chunk_index',0)}"
        seen_ids.add(chunk_id)
        items.append({
            "text": doc,
            "distance": round(dist, 4),
            "article_title": meta.get("article_title", ""),
            "article_url": meta.get("article_url", ""),
            "published_at": meta.get("published_at", ""),
            "chunk_index": meta.get("chunk_index", 0),
            "total_chunks": meta.get("total_chunks", 0),
        })

    # ã‚¿ã‚¤ãƒˆãƒ«éƒ¨åˆ†ä¸€è‡´ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§æ¼ã‚ŒãŸè¨˜äº‹ã‚’è£œå®Œ
    try:
        all_meta = _collection.get(include=["metadatas", "documents"])
        title_matches = []
        for doc, meta in zip(all_meta["documents"], all_meta["metadatas"]):
            chunk_id = f"{meta.get('article_key','')}_{meta.get('chunk_index',0)}"
            if chunk_id not in seen_ids and query in meta.get("article_title", ""):
                title_matches.append({
                    "text": doc,
                    "distance": 0.0,  # ã‚¿ã‚¤ãƒˆãƒ«å®Œå…¨ä¸€è‡´ã¯distance 0æ‰±ã„
                    "article_title": meta.get("article_title", ""),
                    "article_url": meta.get("article_url", ""),
                    "published_at": meta.get("published_at", ""),
                    "chunk_index": meta.get("chunk_index", 0),
                    "total_chunks": meta.get("total_chunks", 0),
                })
        # ã‚¿ã‚¤ãƒˆãƒ«ãƒãƒƒãƒã‚’å…ˆé ­ã«æŒ¿å…¥ï¼ˆæœ€å¤§n_resultsä»¶ã«åã‚ã‚‹ï¼‰
        if title_matches:
            items = title_matches[:n_results] + items
            items = items[:n_results]
    except Exception:
        pass  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¤±æ•—æ™‚ã¯ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœã®ã¿è¿”ã™

    return JSONResponse({"query": query, "results": items})
